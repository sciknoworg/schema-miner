Reference,Comparison,rouge1,rouge2,rougeL,rougeLsum,bleu_bleu,bleu_precisions,bleu_brevity_penalty,bleu_length_ratio,bleu_translation_length,bleu_reference_length,bert_precision,bert_recall,bert_f1
gpt-4-turbo,gpt-4o,0.651933702,0.334440753,0.369060773,0.369060773,0.415103222,"[0.7217090069284064, 0.484393063583815, 0.3506944444444444, 0.2421784472769409]",1,1.002314815,866,864,0.809102535,0.800513744,0.804662963
,meta-llama-3.1-8b-instruct,0.542168675,0.236714976,0.289156627,0.289156627,0.239251929,"[0.6313497822931785, 0.3866279069767442, 0.25181950509461426, 0.14723032069970846]",0.775698557,0.797453704,689,864,0.771515946,0.741381665,0.756064395
gpt-4o,gpt-4-turbo,0.651933702,0.334440753,0.369060773,0.369060773,0.41510378,"[0.7233796296296297, 0.4855156431054461, 0.351508120649652, 0.24274099883855982]",0.997687862,0.997690531,864,866,0.800513744,0.809102535,0.804662963
,meta-llama-3.1-8b-instruct,0.539765319,0.235294118,0.333767927,0.333767927,0.339790219,"[0.7097242380261248, 0.5130813953488372, 0.3813682678311499, 0.26822157434402333]",0.773450157,0.795612009,689,866,0.781096776,0.76326559,0.771681229
meta-llama-3.1-8b-instruct,gpt-4-turbo,0.542168675,0.236714976,0.289156627,0.289156627,0.245853193,"[0.5034722222222222, 0.3082271147161066, 0.20069605568445475, 0.1173054587688734]",1,1.253991292,864,689,0.741381665,0.771515965,0.756064415
,gpt-4o,0.539765319,0.235294118,0.333767927,0.333767927,0.349370342,"[0.5646651270207852, 0.40809248554913297, 0.30324074074074076, 0.21320973348783315]",1,1.256894049,866,689,0.76326561,0.781096776,0.771681209
